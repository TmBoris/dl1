{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:43.714829Z","iopub.status.busy":"2024-03-08T16:45:43.713676Z","iopub.status.idle":"2024-03-08T16:45:56.651557Z","shell.execute_reply":"2024-03-08T16:45:56.650424Z","shell.execute_reply.started":"2024-03-08T16:45:43.714796Z"},"trusted":true},"outputs":[],"source":["!pip install sacrebleu"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.654485Z","iopub.status.busy":"2024-03-08T16:45:56.654074Z","iopub.status.idle":"2024-03-08T16:45:56.661841Z","shell.execute_reply":"2024-03-08T16:45:56.660809Z","shell.execute_reply.started":"2024-03-08T16:45:56.654450Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from torch import nn\n","from tqdm.notebook import tqdm\n","from torchtext.vocab import build_vocab_from_iterator\n","import math, copy, time\n","from torch.autograd import Variable\n","from torch.utils.data import TensorDataset, DataLoader\n","import wandb\n","import sacrebleu\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{},"source":["## Tokenization"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.663185Z","iopub.status.busy":"2024-03-08T16:45:56.662849Z","iopub.status.idle":"2024-03-08T16:45:56.680156Z","shell.execute_reply":"2024-03-08T16:45:56.679285Z","shell.execute_reply.started":"2024-03-08T16:45:56.663149Z"},"trusted":true},"outputs":[],"source":["def dataset_iterator(path):\n","    with open(path) as texts:\n","        for text in texts:\n","            yield text.split()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.683286Z","iopub.status.busy":"2024-03-08T16:45:56.682850Z","iopub.status.idle":"2024-03-08T16:45:56.692031Z","shell.execute_reply":"2024-03-08T16:45:56.691154Z","shell.execute_reply.started":"2024-03-08T16:45:56.683252Z"},"trusted":true},"outputs":[],"source":["def build_vocab(path, min_freq):\n","    return build_vocab_from_iterator(\n","        dataset_iterator(path),\n","        specials=['<pad>', '<unk>', '<bos>', '<eos>'], min_freq=min_freq,\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.693952Z","iopub.status.busy":"2024-03-08T16:45:56.693443Z","iopub.status.idle":"2024-03-08T16:45:56.711372Z","shell.execute_reply":"2024-03-08T16:45:56.710455Z","shell.execute_reply.started":"2024-03-08T16:45:56.693925Z"},"trusted":true},"outputs":[],"source":["def get_train_val_dataloaders(path: str, en_vocab, de_vocab, batch_size: int = 216) -> list[DataLoader]:\n","    '''\n","    Creating train and validation dataloaders from files with plain text.\n","    :param path: path to the folder with train and validation files\n","    :param batch_size: size of the batch in dataloader\n","    :return: train_loader, val_loader\n","    '''\n","\n","    # tokenization of sequences\n","    # en_vocab['<bos>'] == 2\n","    # en_vocab['<eos>'] == 3\n","    train_en_tokens = []\n","    for text in dataset_iterator(f'{path}/data/train.de-en.en'):\n","        tokens = [2] + [en_vocab[word] if word in en_vocab else en_vocab['<unk>'] for word in text] + [3]\n","        train_en_tokens += [tokens]\n","\n","    train_de_tokens = []\n","    for text in dataset_iterator(f'{path}/data/train.de-en.de'):\n","        tokens = [2] + [de_vocab[word] if word in de_vocab else de_vocab['<unk>'] for word in text] + [3]\n","        train_de_tokens += [tokens]\n","\n","    test_en_tokens = []\n","    for text in dataset_iterator(f'{path}/data//val.de-en.en'):\n","        tokens = [2] + [en_vocab[word] if word in en_vocab else en_vocab['<unk>'] for word in text] + [3]\n","        test_en_tokens += [tokens]\n","\n","    test_de_tokens = []\n","    for text in dataset_iterator(f'{path}/data//val.de-en.de'):\n","        tokens = [2] + [de_vocab[word] if word in de_vocab else de_vocab['<unk>'] for word in text] + [3]\n","        test_de_tokens += [tokens]\n","\n","\n","    # padding till max length of sequence\n","    max_length = 82\n","\n","    tokenized_en_train = torch.full((len(train_en_tokens), max_length), en_vocab['<pad>'], dtype=torch.int32)\n","    for i, tokens in enumerate(train_en_tokens):\n","        length = min(max_length, len(tokens))\n","        tokenized_en_train[i, :length] = torch.tensor(tokens[:length])\n","\n","    tokenized_de_train = torch.full((len(train_de_tokens), max_length), de_vocab['<pad>'], dtype=torch.int32)\n","    for i, tokens in enumerate(train_de_tokens):\n","        length = min(max_length, len(tokens))\n","        tokenized_de_train[i, :length] = torch.tensor(tokens[:length])\n","\n","    tokenized_en_test = torch.full((len(test_en_tokens), max_length), en_vocab['<pad>'], dtype=torch.int32)\n","    for i, tokens in enumerate(test_en_tokens):\n","        length = min(max_length, len(tokens))\n","        tokenized_en_test[i, :length] = torch.tensor(tokens[:length])\n","\n","    tokenized_de_test = torch.full((len(test_de_tokens), max_length), de_vocab['<pad>'], dtype=torch.int32)\n","    for i, tokens in enumerate(test_de_tokens):\n","        length = min(max_length, len(tokens))\n","        tokenized_de_test[i, :length] = torch.tensor(tokens[:length])\n","\n","    # creating dataloaders\n","    train_dataset = TensorDataset(tokenized_de_train, tokenized_en_train) # for sync shuffle\n","    test_dataset = TensorDataset(tokenized_de_test, tokenized_en_test)\n","\n","    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n","    val_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n","\n","    return train_loader, val_loader\n","        "]},{"cell_type":"markdown","metadata":{},"source":["## Model"]},{"cell_type":"markdown","metadata":{},"source":["### Common utils"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.712777Z","iopub.status.busy":"2024-03-08T16:45:56.712498Z","iopub.status.idle":"2024-03-08T16:45:56.726659Z","shell.execute_reply":"2024-03-08T16:45:56.725754Z","shell.execute_reply.started":"2024-03-08T16:45:56.712754Z"},"trusted":true},"outputs":[],"source":["def clones(module: nn.Module, N: int) -> nn.ModuleList:\n","    '''\n","    Produce N identical layers.\n","    :param module: module that we want to copy\n","    :param N: number of copies that we want\n","    :return: N copies of module\n","    '''\n","    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.728141Z","iopub.status.busy":"2024-03-08T16:45:56.727774Z","iopub.status.idle":"2024-03-08T16:45:56.737945Z","shell.execute_reply":"2024-03-08T16:45:56.737105Z","shell.execute_reply.started":"2024-03-08T16:45:56.728099Z"},"trusted":true},"outputs":[],"source":["class LayerNorm(nn.Module):\n","    \"Construct a layernorm module.\"\n","    def __init__(self, size: int, eps: float =1e-6) -> None:\n","        '''\n","        :param size: size of embeddings that we will get\n","        :param eps: small constant\n","        '''\n","        super(LayerNorm, self).__init__()\n","        self.a_2 = nn.Parameter(torch.ones(size))\n","        self.b_2 = nn.Parameter(torch.zeros(size))\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        '''\n","        :param x: input batch\n","        '''\n","        mean = x.mean(-1, keepdim=True)\n","        std = x.std(-1, keepdim=True)\n","        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.739603Z","iopub.status.busy":"2024-03-08T16:45:56.739274Z","iopub.status.idle":"2024-03-08T16:45:56.754194Z","shell.execute_reply":"2024-03-08T16:45:56.753286Z","shell.execute_reply.started":"2024-03-08T16:45:56.739565Z"},"trusted":true},"outputs":[],"source":["class SublayerConnection(nn.Module):\n","    \"\"\"\n","    A residual connection followed by a layer norm.\n","    \"\"\"\n","    def __init__(self, size: int, dropout: float):\n","        '''\n","        :param size: size of layernorm\n","        :param dropout: dropout parameter\n","        '''\n","        super(SublayerConnection, self).__init__()\n","        self.norm = LayerNorm(size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, sublayer):\n","        '''\n","        Apply residual connection to any sublayer with the same size.\n","        :param x: input of the layer\n","        :param sublayer: implementation of layer, which we wrap into residual connection\n","        :return: embedding of the same shape as x\n","        '''\n","        return x + self.dropout(sublayer(self.norm(x)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.755478Z","iopub.status.busy":"2024-03-08T16:45:56.755223Z","iopub.status.idle":"2024-03-08T16:45:56.764188Z","shell.execute_reply":"2024-03-08T16:45:56.763425Z","shell.execute_reply.started":"2024-03-08T16:45:56.755457Z"},"trusted":true},"outputs":[],"source":["def subsequent_mask(size: int):\n","    '''\n","    Mask out subsequent positions.\n","    :param size: size of attention layer\n","    :return: mask\n","    '''\n","    attn_shape = (1, size, size)\n","    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n","    return torch.from_numpy(subsequent_mask) == 0"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.769005Z","iopub.status.busy":"2024-03-08T16:45:56.768563Z","iopub.status.idle":"2024-03-08T16:45:56.776802Z","shell.execute_reply":"2024-03-08T16:45:56.775854Z","shell.execute_reply.started":"2024-03-08T16:45:56.768958Z"},"trusted":true},"outputs":[],"source":["class Batch:\n","    \"Object for holding a batch of data with mask during training.\"\n","    def __init__(self, src, trg=None, pad=0):\n","        self.src = src\n","        self.src_mask = (src != pad).unsqueeze(-2)\n","        if trg is not None:\n","            self.trg = trg[:, :-1]\n","            self.trg_y = trg[:, 1:]\n","            self.trg_mask = \\\n","                self.make_std_mask(self.trg, pad)\n","            self.ntokens = (self.trg_y != pad).data.sum()\n","    \n","    def make_std_mask(self, tgt, pad):\n","        \"Create a mask to hide padding and future words.\"\n","        tgt_mask = (tgt != pad).unsqueeze(-2)\n","        tgt_mask = tgt_mask & Variable(\n","            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n","        return tgt_mask"]},{"cell_type":"markdown","metadata":{},"source":["### Embeddings and Softmax"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.778364Z","iopub.status.busy":"2024-03-08T16:45:56.778030Z","iopub.status.idle":"2024-03-08T16:45:56.792388Z","shell.execute_reply":"2024-03-08T16:45:56.791613Z","shell.execute_reply.started":"2024-03-08T16:45:56.778332Z"},"trusted":true},"outputs":[],"source":["class Embeddings(nn.Module):\n","    \"Wrapper for embedding module\"\n","    def __init__(self, d_model: int, vocab_size: int):\n","        '''\n","        :param d_model: size of embeddings\n","        :param vocab_size: size of vocab\n","        '''\n","        super(Embeddings, self).__init__()\n","        self.emb = nn.Embedding(vocab_size, d_model)\n","        self.d_model = d_model\n","\n","    def forward(self, x):\n","        \"translating into embedding and normalize\"\n","        return self.emb(x) * math.sqrt(self.d_model)\n","    "]},{"cell_type":"markdown","metadata":{},"source":["### Feed-Forward"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.793750Z","iopub.status.busy":"2024-03-08T16:45:56.793466Z","iopub.status.idle":"2024-03-08T16:45:56.803745Z","shell.execute_reply":"2024-03-08T16:45:56.802926Z","shell.execute_reply.started":"2024-03-08T16:45:56.793727Z"},"trusted":true},"outputs":[],"source":["class PositionwiseFeedForward(nn.Module):\n","    \"Implements Feed-Forward layer\"\n","    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n","        '''\n","        :param d_model: size of embeddings\n","        :param d_ff: size of latent represintation between 2 FC layers\n","        '''\n","        super(PositionwiseFeedForward, self).__init__()\n","        self.w_1 = nn.Linear(d_model, d_ff)\n","        self.w_2 = nn.Linear(d_ff, d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        return self.w_2(self.dropout(F.relu(self.w_1(x))))"]},{"cell_type":"markdown","metadata":{},"source":["### Positional Encoding"]},{"cell_type":"markdown","metadata":{},"source":["PE(pos, 2i) = sin(pos / (10000^(2i/d_model)))\n","\n","PE(pos, 2i+1) = cos(pos / (10000^(2i/d_model)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.805684Z","iopub.status.busy":"2024-03-08T16:45:56.804917Z","iopub.status.idle":"2024-03-08T16:45:56.816363Z","shell.execute_reply":"2024-03-08T16:45:56.815339Z","shell.execute_reply.started":"2024-03-08T16:45:56.805651Z"},"trusted":true},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    \"Implement the PE function.\"\n","    def __init__(self, d_model, dropout, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        \n","        # Compute the positional encodings\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) *\n","                             -(math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","        \n","    def forward(self, x):\n","        x = x + Variable(self.pe[:, :x.size(1)], \n","                         requires_grad=False)\n","        return self.dropout(x)"]},{"cell_type":"markdown","metadata":{},"source":["### Attention"]},{"cell_type":"markdown","metadata":{},"source":["Attention(Q,K,V)=softmax(QK^T/sqrt(d_k)) * V"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.817855Z","iopub.status.busy":"2024-03-08T16:45:56.817521Z","iopub.status.idle":"2024-03-08T16:45:56.830711Z","shell.execute_reply":"2024-03-08T16:45:56.829735Z","shell.execute_reply.started":"2024-03-08T16:45:56.817825Z"},"trusted":true},"outputs":[],"source":["def Attention(query, key, value, mask=None, dropout=None):\n","    '''\n","    Attention is all you need.\n","    :param query: embeds responsible for the query\n","    :param key: embeds responsible for the key\n","    :param value: embeds responsible for the value\n","    '''\n","    d_k = query.size(-1) # embed dim\n","    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n","             / math.sqrt(d_k)\n","    if mask is not None:\n","        scores = scores.masked_fill(mask == 0, -1e9)\n","    p_attn = F.softmax(scores, dim = -1)\n","    if dropout is not None:\n","        p_attn = dropout(p_attn)\n","    return torch.matmul(p_attn, value)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.832173Z","iopub.status.busy":"2024-03-08T16:45:56.831854Z","iopub.status.idle":"2024-03-08T16:45:56.843262Z","shell.execute_reply":"2024-03-08T16:45:56.842353Z","shell.execute_reply.started":"2024-03-08T16:45:56.832150Z"},"trusted":true},"outputs":[],"source":["\n","class MultiHeadedAttention(nn.Module):\n","    def __init__(self, h: int, d_model: int, dropout: float = 0.1):\n","        '''\n","        Take in model size and number of heads.\n","        :param h: number of heads\n","        :param d_model: dimension of embeddings\n","        :param dropout: dropout probability\n","        '''\n","        super(MultiHeadedAttention, self).__init__()\n","        assert d_model % h == 0\n","        # We assume d_v always equals d_k\n","        self.d_k = d_model // h\n","        self.h = h\n","        self.linears = clones(nn.Linear(d_model, d_model), 4)\n","        self.dropout = nn.Dropout(p=dropout)\n","        \n","    def forward(self, query, key, value, mask=None):\n","        if mask is not None:\n","            # Same mask applied to all h heads.\n","            mask = mask.unsqueeze(1)\n","        batch_size = query.size(0)\n","        \n","        # apply matrices WQ, WV, WK and split into self.h heads\n","        query, key, value = \\\n","            [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n","             for l, x in zip(self.linears, (query, key, value))]\n","        \n","        # apply attention on all heads\n","        x = Attention(query, key, value, mask=mask, \n","                                 dropout=self.dropout)\n","        \n","        # Concat using a view and apply a final linear. \n","        x = x.transpose(1, 2).contiguous() \\\n","             .view(batch_size, -1, self.h * self.d_k)\n","        return self.linears[-1](x)"]},{"cell_type":"markdown","metadata":{},"source":["### encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.844728Z","iopub.status.busy":"2024-03-08T16:45:56.844160Z","iopub.status.idle":"2024-03-08T16:45:56.860328Z","shell.execute_reply":"2024-03-08T16:45:56.859371Z","shell.execute_reply.started":"2024-03-08T16:45:56.844694Z"},"trusted":true},"outputs":[],"source":["class EncoderLayer(nn.Module):\n","    \"Encoder is made up of self-attn and feed forward\"\n","    def __init__(self, size: int, self_attn, feed_forward, dropout):\n","        super(EncoderLayer, self).__init__()\n","        self.self_attn = self_attn\n","        self.feed_forward = feed_forward\n","        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n","        self.size = size\n","\n","    def forward(self, x, mask):\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n","        return self.sublayer[1](x, self.feed_forward)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.861725Z","iopub.status.busy":"2024-03-08T16:45:56.861408Z","iopub.status.idle":"2024-03-08T16:45:56.871774Z","shell.execute_reply":"2024-03-08T16:45:56.871037Z","shell.execute_reply.started":"2024-03-08T16:45:56.861693Z"},"trusted":true},"outputs":[],"source":["class Encoder(nn.Module):\n","    \"Core encoder is a stack of N layers\"\n","    def __init__(self, layer, N):\n","        super(Encoder, self).__init__()\n","        self.layers = clones(layer, N)\n","        self.norm = LayerNorm(layer.size)\n","        \n","    def forward(self, x, mask):\n","        \"Pass the input (and mask) through each layer in turn.\"\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        return self.norm(x)"]},{"cell_type":"markdown","metadata":{},"source":["### decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.873154Z","iopub.status.busy":"2024-03-08T16:45:56.872869Z","iopub.status.idle":"2024-03-08T16:45:56.887111Z","shell.execute_reply":"2024-03-08T16:45:56.886293Z","shell.execute_reply.started":"2024-03-08T16:45:56.873133Z"},"trusted":true},"outputs":[],"source":["class DecoderLayer(nn.Module):\n","    \"Decoder is made of self-attn, src-attn, and feed forward\"\n","    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n","        super(DecoderLayer, self).__init__()\n","        self.size = size\n","        self.self_attn = self_attn\n","        self.src_attn = src_attn\n","        self.feed_forward = feed_forward\n","        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n"," \n","    def forward(self, x, memory, src_mask, tgt_mask):\n","        m = memory\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n","        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n","        return self.sublayer[2](x, self.feed_forward)\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.888355Z","iopub.status.busy":"2024-03-08T16:45:56.888113Z","iopub.status.idle":"2024-03-08T16:45:56.898620Z","shell.execute_reply":"2024-03-08T16:45:56.897738Z","shell.execute_reply.started":"2024-03-08T16:45:56.888335Z"},"trusted":true},"outputs":[],"source":["class Decoder(nn.Module):\n","    \"Generic N layer decoder with masking.\"\n","    def __init__(self, layer, N):\n","        super(Decoder, self).__init__()\n","        self.layers = clones(layer, N)\n","        self.norm = LayerNorm(layer.size)\n","        \n","    def forward(self, x, memory, src_mask, tgt_mask):\n","        for layer in self.layers:\n","            x = layer(x, memory, src_mask, tgt_mask)\n","        return self.norm(x)"]},{"cell_type":"markdown","metadata":{},"source":["### EncoderDecoder"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.900180Z","iopub.status.busy":"2024-03-08T16:45:56.899798Z","iopub.status.idle":"2024-03-08T16:45:56.911118Z","shell.execute_reply":"2024-03-08T16:45:56.910269Z","shell.execute_reply.started":"2024-03-08T16:45:56.900124Z"},"trusted":true},"outputs":[],"source":["class Transformer(nn.Module):\n","    \"\"\"\n","    A standard Encoder-Decoder architecture.\n","    \"\"\"\n","    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n","        super(Transformer, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.src_embed = src_embed\n","        self.tgt_embed = tgt_embed\n","        self.generator = generator\n","        \n","    def forward(self, src, tgt, src_mask, tgt_mask):\n","        \"Take in and process masked src and target sequences.\"\n","        return self.decode(self.encode(src, src_mask), src_mask,\n","                            tgt, tgt_mask)\n","    \n","    def encode(self, src, src_mask):\n","        return self.encoder(self.src_embed(src), src_mask)\n","    \n","    def decode(self, memory, src_mask, tgt, tgt_mask):\n","        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.912601Z","iopub.status.busy":"2024-03-08T16:45:56.912279Z","iopub.status.idle":"2024-03-08T16:45:56.926187Z","shell.execute_reply":"2024-03-08T16:45:56.925405Z","shell.execute_reply.started":"2024-03-08T16:45:56.912578Z"},"trusted":true},"outputs":[],"source":["class Generator(nn.Module):\n","    \"Last FC layer.\"\n","    def __init__(self, d_model, vocab):\n","        super(Generator, self).__init__()\n","        self.linear = nn.Linear(d_model, vocab)\n","\n","    def forward(self, x):\n","        return self.linear(x)"]},{"cell_type":"markdown","metadata":{},"source":["### Full Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.927529Z","iopub.status.busy":"2024-03-08T16:45:56.927241Z","iopub.status.idle":"2024-03-08T16:45:56.938223Z","shell.execute_reply":"2024-03-08T16:45:56.937453Z","shell.execute_reply.started":"2024-03-08T16:45:56.927488Z"},"trusted":true},"outputs":[],"source":["def make_transformer(src_vocab, tgt_vocab, N=6, \n","               d_model=512, d_ff=2048, h=8, dropout=0.1):\n","    \"Construct a model from hyperparameters.\"\n","    c = copy.deepcopy\n","    attn = MultiHeadedAttention(h, d_model)\n","    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n","    position = PositionalEncoding(d_model, dropout)\n","    model = Transformer(\n","        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n","        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n","                             c(ff), dropout), N),\n","        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n","        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n","        Generator(d_model, tgt_vocab))\n","    \n","    # Initialize parameters with Xavier distribution\n","    for p in model.parameters():\n","        if p.dim() > 1:\n","            nn.init.xavier_uniform_(p)\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"markdown","metadata":{},"source":["### Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.939827Z","iopub.status.busy":"2024-03-08T16:45:56.939552Z","iopub.status.idle":"2024-03-08T16:45:56.951582Z","shell.execute_reply":"2024-03-08T16:45:56.950692Z","shell.execute_reply.started":"2024-03-08T16:45:56.939805Z"},"trusted":true},"outputs":[],"source":["class WarmupOptimizer:\n","    \"Optim wrapper that implements rate.\"\n","    def __init__(self, model_size, factor, warmup, optimizer):\n","        self.optimizer = optimizer\n","        self._step = 0\n","        self.warmup = warmup\n","        self.factor = factor\n","        self.model_size = model_size\n","        self._rate = 0\n","        \n","    def step(self):\n","        \"Update parameters and rate\"\n","        self._step += 1\n","        rate = self.rate()\n","        for p in self.optimizer.param_groups:\n","            p['lr'] = rate\n","        self._rate = rate\n","        self.optimizer.step()\n","        \n","    def rate(self, step = None):\n","        \"Implement `lrate` above\"\n","        if step is None:\n","            step = self._step\n","        return self.factor * \\\n","            (self.model_size ** (-0.5) *\n","            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n"]},{"cell_type":"markdown","metadata":{},"source":["### Loss computation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.952892Z","iopub.status.busy":"2024-03-08T16:45:56.952613Z","iopub.status.idle":"2024-03-08T16:45:56.967211Z","shell.execute_reply":"2024-03-08T16:45:56.966202Z","shell.execute_reply.started":"2024-03-08T16:45:56.952870Z"},"trusted":true},"outputs":[],"source":["class LossBLEUCompute:\n","    \"A simple loss compute and train function.\"\n","    def __init__(self, generator, criterion, en_vocab_reversed, calc_bleu, opt=None):\n","        self.generator = generator\n","        self.criterion = criterion\n","        self.en_vocab_reversed = en_vocab_reversed\n","        self.calc_bleu = calc_bleu\n","        self.opt = opt\n","    \n","        \n","    def __call__(self, x, y):\n","        x = self.generator(x)\n","        bleu = 0\n","        if self.calc_bleu:\n","            next_tokens = torch.argmax(x, dim=2)\n","            # B * L * 1\n","\n","            predicted_sentences = [' '.join([self.en_vocab_reversed[word] for word in next_sentence]) + '\\n' for next_sentence in next_tokens]\n","            reference_sentences = [' '.join([self.en_vocab_reversed[word] for word in next_sentence]) + '\\n' for next_sentence in y]\n","            \n","            bleu = sacrebleu.corpus_bleu(reference_sentences, [predicted_sentences]).score\n","        \n","        loss = self.criterion(x.transpose(1, 2), \n","                              y.long())\n","\n","        loss.backward()\n","        \n","        if self.opt is not None:\n","            self.opt.step()\n","#             self.opt.optimizer.zero_grad()\n","            self.opt.zero_grad()\n","\n","        return loss.item(), bleu"]},{"cell_type":"markdown","metadata":{},"source":["### Training Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.968836Z","iopub.status.busy":"2024-03-08T16:45:56.968493Z","iopub.status.idle":"2024-03-08T16:45:56.979402Z","shell.execute_reply":"2024-03-08T16:45:56.978463Z","shell.execute_reply.started":"2024-03-08T16:45:56.968804Z"},"trusted":true},"outputs":[],"source":["def run_epoch(data_loader, model, loss_bleu_compute):\n","    \"Standard Training and Logging Function\"\n","    start = time.time()\n","    total_tokens = 1\n","    total_loss = 0\n","    total_bleu = 0\n","    tokens = 0\n","    for i, src_trg in tqdm(enumerate(data_loader)):\n","        src, trg = src_trg\n","        batch = Batch(src, trg, 0)\n","        batch.src = batch.src.to(device)\n","        batch.trg = batch.trg.to(device)\n","        batch.src_mask = batch.src_mask.to(device)\n","        batch.trg_mask = batch.trg_mask.to(device)\n","        batch.trg_y = batch.trg_y.to(device)\n","        batch.ntokens = batch.ntokens.to(device)\n","        \n","        out = model.forward(batch.src, batch.trg, \n","                            batch.src_mask, batch.trg_mask)\n","        loss, bleu = loss_bleu_compute(out, batch.trg_y)\n","        total_loss += loss\n","        total_bleu += bleu\n","        total_tokens += batch.ntokens\n","        tokens += batch.ntokens\n","        if i % 200 == 1:\n","            elapsed = time.time() - start\n","#             print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n","#                     (i, loss / batch.ntokens, tokens / elapsed))\n","            start = time.time()\n","            tokens = 0\n","    return total_loss / total_tokens, total_bleu / total_tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:56.981184Z","iopub.status.busy":"2024-03-08T16:45:56.980762Z","iopub.status.idle":"2024-03-08T16:45:56.995285Z","shell.execute_reply":"2024-03-08T16:45:56.994460Z","shell.execute_reply.started":"2024-03-08T16:45:56.981157Z"},"trusted":true},"outputs":[],"source":["def train_loop(num_epochs, model, train_loader, test_loader, train_loss_comp, test_loss_comp, config):\n","    wandb.login(key='WANDB_PROFILE_KEY')\n","    wandb.init(\n","        project='DL_BHW_2',\n","        entity='bspanfilov',\n","        name=config['run_name'],\n","        config=config\n","    )\n","    train_loss, val_loss = [], []\n","    train_bleu, val_bleu = [], []\n","    for epoch in range(num_epochs):\n","        model.train()\n","        loss, bleu = run_epoch(train_loader, model, train_loss_comp)\n","        train_loss.append(loss)\n","        train_bleu.append(bleu)\n","        model.eval()\n","        loss, bleu = run_epoch(test_loader, model, test_loss_comp)\n","        val_loss.append(loss)\n","        val_bleu.append(bleu)\n","#         print('DELETE it after self-check')\n","#         print(f'Train loss after {epoch}th epoch: {train_loss[-1]}')\n","#         print(f'Val loss after {epoch}th epoch: {val_loss[-1]}')\n","\n","        wandb.log({\"train_loss\": train_loss[-1], \"val_loss\": val_loss[-1]})\n","        wandb.log({\"train_bleu\": train_bleu[-1], \"val_bleu\": val_bleu[-1]})\n","\n","    wandb.finish()"]},{"cell_type":"markdown","metadata":{},"source":["## Inference\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T20:47:57.929273Z","iopub.status.busy":"2024-03-08T20:47:57.928130Z","iopub.status.idle":"2024-03-08T20:47:57.947173Z","shell.execute_reply":"2024-03-08T20:47:57.946170Z","shell.execute_reply.started":"2024-03-08T20:47:57.929204Z"},"trusted":true},"outputs":[],"source":["@torch.no_grad()\n","def beam_search(model, tokenized_src, en_vocab, max_length=90, beam_width=3):\n","    assert beam_width > 0\n","\n","    model.eval()\n","    src = torch.tensor([tokenized_src]).to(device)\n","    memory = model.encode(src, None)\n","    trg_tokens = [en_vocab[\"<bos>\"]]\n","    beams = [(trg_tokens, 0)]  # Список кортежей (токены, счет)\n","\n","    ans_beams = []\n","    \n","    for curr_l in range(max_length):\n","        new_beams = []\n","        for beam_tokens, beam_score in beams:\n","\n","            if beam_tokens[-1] == en_vocab[\"<eos>\"]:\n","                ans_beams.append((beam_tokens, beam_score * float(curr_l) ** (-0.75)))\n","                continue\n","            trg = torch.tensor([beam_tokens]).to(device)\n","\n","            with torch.no_grad():\n","                output = model.decode(memory, None, trg, None)\n","                \n","            prob_distribution = model.generator(output[:, -1])\n","            top_scores, top_prob_tokens = torch.topk(prob_distribution, beam_width)\n","\n","            for score, tokens in zip(top_scores.squeeze(), top_prob_tokens.squeeze()):\n","                new_score = beam_score - torch.log(score).item()  # Учет вероятности\n","                new_beam = (beam_tokens + [tokens.item()], new_score)\n","                new_beams.append(new_beam)\n","        if len(ans_beams) >= beam_width:\n","            break\n","\n","        new_beams.sort(key=lambda x: x[1])\n","\n","        beams = new_beams[:beam_width]\n","\n","    ans_beams.sort(key=lambda x: x[1])\n","    best_beam_tokens, _ = ans_beams[0]\n","    return best_beam_tokens[1:-1]  # Удаляем <bos> и <eos>\n","\n","def inference_loop_beam_search(model, tokenized_src, en_vocab, max_length=90, beam_width=5):\n","    translated_sentence = beam_search(model, tokenized_src, en_vocab, max_length, beam_width)\n","    en_reverse_vocab = en_vocab.get_itos()\n","    translated_sentence = [en_reverse_vocab[token] for token in translated_sentence]\n","    return translated_sentence\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:57.016977Z","iopub.status.busy":"2024-03-08T16:45:57.016588Z","iopub.status.idle":"2024-03-08T16:45:57.027414Z","shell.execute_reply":"2024-03-08T16:45:57.026477Z","shell.execute_reply.started":"2024-03-08T16:45:57.016953Z"},"trusted":true},"outputs":[],"source":["@torch.no_grad()\n","def inference_loop(model, tokenized_src, en_vocab, max_length=90):\n","    en_reverse_vocab = en_vocab.get_itos()\n","\n","    model.eval()\n","    src = torch.tensor([tokenized_src]).to(device)\n","    memory = model.encode(src, None)\n","    trg_tokens = [en_vocab[\"<bos>\"]]\n","    \n","    for _ in range(max_length):\n","        trg = torch.tensor([trg_tokens]).to(device)\n","\n","        with torch.no_grad():\n","            output = model.decode(memory, None, trg, None)\n","\n","        logits = model.generator(output[:, -1, :])\n","        next_token = torch.argmax(logits, dim=1).item()\n","\n","        if next_token == en_vocab[\"<eos>\"]:\n","            break\n","        \n","        trg_tokens.append(next_token)\n","    \n","    return [en_reverse_vocab[token] for token in trg_tokens][1:]"]},{"cell_type":"markdown","metadata":{},"source":["## Experiments"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:57.042142Z","iopub.status.busy":"2024-03-08T16:45:57.041856Z","iopub.status.idle":"2024-03-08T16:45:57.050062Z","shell.execute_reply":"2024-03-08T16:45:57.049116Z","shell.execute_reply.started":"2024-03-08T16:45:57.042119Z"},"trusted":true},"outputs":[],"source":["config = {'model': {'num_layers': 3,\n","  'embedding_dim': 512,\n","  'feedforward_dim': 512,\n","  'num_heads': 8,\n","  'dropout': 0.2},\n"," 'batch_size': 128,\n"," 'optimizer': {'factor': 1,\n","  'warmup': 400,\n","  'lr': 0.0001,\n","  'beta1': 0.9,\n","  'beta2': 0.98},\n"," 'epochs': 30,\n"," 'checkpoint': {'dir': 'checkpoints', 'step': 1},\n"," 'outputfile': '3_512_512_8_without_warmup_beam_search_5.txt',\n"," 'path': '/kaggle/input/bhw2-data',\n"," 'run_name': '3_512_512_8_without_warmup_beam_search_5',\n"," 'min_freq': 10}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:45:57.052122Z","iopub.status.busy":"2024-03-08T16:45:57.051702Z","iopub.status.idle":"2024-03-08T17:29:48.542345Z","shell.execute_reply":"2024-03-08T17:29:48.541403Z","shell.execute_reply.started":"2024-03-08T16:45:57.052092Z"},"trusted":true},"outputs":[],"source":["path = config['path']\n","\n","en_vocab = build_vocab(f'{path}/data/train.de-en.en', config['min_freq'])\n","de_vocab = build_vocab(f'{path}/data/train.de-en.de', config['min_freq'])\n","\n","train_loader, val_loader = \\\n","    get_train_val_dataloaders(path=path, en_vocab=en_vocab,\n","                                de_vocab=de_vocab, batch_size=config['batch_size'])\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=0)\n","model = make_transformer(len(de_vocab), len(en_vocab),\n","                          N=config['model']['num_layers'],\n","                          d_model=config['model']['embedding_dim'],\n","                          d_ff=config['model']['feedforward_dim'], \n","                          h=config['model']['num_heads']).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=config['optimizer']['lr'], \n","                                             betas=(config['optimizer']['beta1'], \n","                                                    config['optimizer']['beta2']), \n","                                             eps=1e-09)\n","num_epochs=config['epochs']\n","\n","\n","train_loop(num_epochs=num_epochs, model=model, train_loader=train_loader,\n","                            test_loader=val_loader,\n","                            train_loss_comp=LossBLEUCompute(model.generator, criterion, en_vocab.get_itos(), calc_bleu=False, opt=optimizer),\n","                            test_loss_comp=LossBLEUCompute(model.generator, criterion, en_vocab.get_itos(), calc_bleu=True, opt=None), config=config)\n","\n","torch.save(model.state_dict(), 'checkpoint_' + config['run_name'] + '_after_' + str(num_epochs) + '_' + 'epochs')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# not optimized, but working one loop for each sequence\n","with open(config['outputfile'] + '_' + 'beam_search_5', 'w') as ans_file:\n","    for text in dataset_iterator(f'{path}/data/test1.de-en.de'):\n","        tokens = [2] + [de_vocab[word] if word in de_vocab else de_vocab['<unk>'] for word in text] + [3]\n","        ans_file.write(' '.join(inference_loop_beam_search(model=model, tokenized_src=tokens, en_vocab=en_vocab, beam_width=5)) + '\\n')"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4487772,"sourceId":7690136,"sourceType":"datasetVersion"},{"datasetId":4514251,"sourceId":7726681,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
